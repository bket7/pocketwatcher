# Recapo / Handoff - 2026-02-05

## Current Status (as of 2026-02-05)
- Worker service is running from this repo (`C:\Users\Administrator\Desktop\Active\pocketwatcher\main.py`) via NSSM.
- System is **processing**, but **throughput is ~320–370 tx/s**, below the 400–600 target.
- Stream backlog is **pinned at 100,000** (`stream:tx`), not draining.
- Swaps are being detected (counters move) and HOT tokens can appear.

## Latest Measured Stats
- `XLEN stream:tx = 100000` (maxed)
- `XPENDING stream:tx parsers` in the **4k–7k** range with 8 consumers
- `tx_per_second_60s` observed: **~324–369 tx/s**
- `swaps_detected_total` increasing steadily

## Current Problems
1. **Throughput still below target**  
   We’re stuck around ~320–370 tx/s. The backlog is not draining.

2. **Stream backlog permanently capped at 100k**  
   Ingest rate is still higher than processing rate, so the stream stays at max length.

3. **Likely CPU/GIL bound**  
   More async consumers within a single process doesn’t scale linearly. This suggests CPU-bound parsing/inference inside a single Python process.

4. **Operational confusion (stream name mismatch)**  
   Runbook refers to `pocketwatcher:txs`, but code uses `stream:tx`.  
   Use `stream:tx` for backlog/pending checks.

## Recent Fixes Already Applied
- Batch processor made re-entrant (no shared mutable batch state).
- Batch consumer hardens missing signatures and uses message ID fallback.
- `_tx_to_dict` now preserves signature even on partial parse errors.
- Delta log cleanup skips the currently open file to avoid Windows lock spam.
- ACKs merged into the same Redis pipeline as counter updates.
- Dedup now uses local TTL cache in `stream/dedup.py`.
- Detection throttling under critical backlog.
- Increased batch consumers to 8 (from max 4).
- Micro-optimizations in swap inference (pre-group by owner).

## What’s Working
- Consumers are active (pending messages spread across `batch-1..batch-8`).
- Swaps detected increase (counters move).
- No fatal errors in worker logs since the restarts.

## What’s Not Working Yet
- **Backlog isn’t draining** at current ingestion rate.
- **Throughput is below target**, even after optimizations.

## Recommended Next Step (Big Fix)
**Split ingest and processing into separate processes**:
1. Add CLI modes:
   - `--ingest-only` (Yellowstone -> Redis)
   - `--consume-only` (Redis -> BatchConsumer)
2. Run:
   - 1 ingest process
   - 2–4 consume processes (parallel CPU usage)
3. Keep alerts/detection in one process to avoid duplicates.

This is the most likely path to **400–600 tx/s** on a multi-core machine.

## Commands Used for Verification
```powershell
# Stream backlog
python -c "import redis, os, json; r=redis.Redis.from_url('redis://localhost:6379/0', decode_responses=True); print(r.xlen('stream:tx'))"

# Pending messages
python -c "import redis; r=redis.Redis.from_url('redis://localhost:6379/0', decode_responses=True); print(r.xpending('stream:tx','parsers'))"

# Live stats
python -c "import redis, json; r=redis.Redis.from_url('redis://localhost:6379/0', decode_responses=True); print(json.loads(r.get('pocketwatcher:live_stats')))"
```

## Files Changed This Session
- `core/batch_processor.py`
- `stream/batch_consumer.py`
- `stream/dedup.py`
- `main.py`
- `parser/inference.py`
- `storage/delta_log.py`
- `docs/HANDOFF_2026-02-04.md`

